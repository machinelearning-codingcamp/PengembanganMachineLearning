Multinomial Naive Bayes Results:
Accuracy: 0.7564
Training time: 0.00 seconds
              precision    recall  f1-score   support

        good       0.76      0.93      0.84      1392
     neutral       0.72      0.38      0.50       305
         bad       0.73      0.47      0.57       450

    accuracy                           0.76      2147
   macro avg       0.74      0.59      0.64      2147
weighted avg       0.75      0.76      0.74      2147


Logistic Regression Results:
Accuracy: 0.8291
Training time: 0.11 seconds
              precision    recall  f1-score   support

        good       0.89      0.90      0.89      1392
     neutral       0.70      0.75      0.72       305
         bad       0.73      0.68      0.71       450

    accuracy                           0.83      2147
   macro avg       0.77      0.77      0.77      2147
weighted avg       0.83      0.83      0.83      2147


Linear SVM Results:
Accuracy: 0.8347
Training time: 0.17 seconds
              precision    recall  f1-score   support

        good       0.91      0.89      0.90      1392
     neutral       0.70      0.77      0.73       305
         bad       0.71      0.72      0.72       450

    accuracy                           0.83      2147
   macro avg       0.77      0.79      0.78      2147
weighted avg       0.84      0.83      0.84      2147


Random Forest Results:
Accuracy: 0.7564
Training time: 2.52 seconds
              precision    recall  f1-score   support

        good       0.89      0.81      0.85      1392
     neutral       0.49      0.86      0.62       305
         bad       0.69      0.53      0.60       450

    accuracy                           0.76      2147
   macro avg       0.69      0.73      0.69      2147
weighted avg       0.79      0.76      0.76      2147


Gradient Boosting Results:
Accuracy: 0.7820
Training time: 81.43 seconds
              precision    recall  f1-score   support

        good       0.78      0.94      0.86      1392
     neutral       0.78      0.45      0.57       305
         bad       0.77      0.51      0.61       450

    accuracy                           0.78      2147
   macro avg       0.78      0.63      0.68      2147
weighted avg       0.78      0.78      0.76      2147

/home/nadia/Documents/non-kuliah/CODINGCAMP/Rottentomatoes/rottenTomatoes/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [20:15:22] WARNING: /workspace/src/learner.cc:740: 
Parameters: { "use_label_encoder" } are not used.

  warnings.warn(smsg, UserWarning)

XGBoost Results:
Accuracy: 0.8095
Training time: 30.79 seconds
              precision    recall  f1-score   support

        good       0.84      0.92      0.88      1392
     neutral       0.68      0.78      0.73       305
         bad       0.78      0.50      0.61       450

    accuracy                           0.81      2147
   macro avg       0.77      0.73      0.74      2147
weighted avg       0.81      0.81      0.80      2147


==== Model Comparison Summary ====
Linear SVM           | Accuracy: 0.8347 | Time: 0.17s
Logistic Regression  | Accuracy: 0.8291 | Time: 0.11s
XGBoost              | Accuracy: 0.8095 | Time: 30.79s
Gradient Boosting    | Accuracy: 0.7820 | Time: 81.43s
Multinomial Naive Bayes | Accuracy: 0.7564 | Time: 0.00s
Random Forest        | Accuracy: 0.7564 | Time: 2.52s

==== Ensemble (Majority Voting) Results ====
Accuracy: 0.8244
              precision    recall  f1-score   support

        good       0.85      0.93      0.89      1392
     neutral       0.73      0.71      0.72       305
         bad       0.81      0.56      0.66       450

    accuracy                           0.82      2147
   macro avg       0.79      0.74      0.76      2147
weighted avg       0.82      0.82      0.82      2147


Model and vectorizer saved. You can use them for predictions:
model = joblib.load('best_model.pkl')
vectorizer = joblib.load('tfidf_vectorizer.pkl')
predictions = model.predict(vectorizer.transform(new_reviews))
