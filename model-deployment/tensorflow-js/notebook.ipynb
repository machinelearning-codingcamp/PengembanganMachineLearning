{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/home/nadia/Documents/non-kuliah/CODINGCAMP/MachineLearningPlayGround/PengembanganMachineLearning/model-deployment/data/archive/sentiment labelled sentences/yelp_labelled.txt', names=['sentence', 'label'], sep='\\t')\n",
    "df = pd.read_csv('data/archive/sentiment labelled sentences/yelp_labelled.txt', names=['sentence', 'label'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wow... loved place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crust good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tasty texture nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stopped late may bank holiday rick steve recom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>selection menu great prices.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>think food flavor texture lacking.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>overall impressed would go back.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>whole experience underwhelming, think go ninja...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>then, wasted enough life there, poured salt wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  label\n",
       "0                                  wow... loved place.      1\n",
       "1                                          crust good.      0\n",
       "2                                 tasty texture nasty.      0\n",
       "3    stopped late may bank holiday rick steve recom...      1\n",
       "4                         selection menu great prices.      1\n",
       "..                                                 ...    ...\n",
       "995                 think food flavor texture lacking.      0\n",
       "996                           appetite instantly gone.      0\n",
       "997                   overall impressed would go back.      0\n",
       "998  whole experience underwhelming, think go ninja...      0\n",
       "999  then, wasted enough life there, poured salt wo...      0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:16: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\]'\n",
      "/tmp/ipykernel_17010/899678979.py:16: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  filt = '!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ' # Filter untuk menghilangkan symbols\n"
     ]
    }
   ],
   "source": [
    "# Mengubah seluruh text kedalam bentuk lowercase\n",
    "df['sentence'] = df['sentence'].str.lower()\n",
    " \n",
    "# Menghilangkan stopwords\n",
    "stop_word = set(stopwords.words('english'))\n",
    " \n",
    "df['sentence'] = df['sentence'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop_word)]))\n",
    " \n",
    "# Melakukan split dataset\n",
    "sentence = df['sentence'].values\n",
    "label = df['label'].values\n",
    " \n",
    "sentence_train, sentence_test, label_train, label_test = train_test_split(sentence, label, test_size=0.2, shuffle=False)\n",
    " \n",
    "# Membuat tokenisasi\n",
    "filt = '!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ' # Filter untuk menghilangkan symbols\n",
    " \n",
    "tokenizer = Tokenizer(num_words=2000, oov_token=\"<OOV>\", filters=filt)\n",
    " \n",
    "tokenizer.fit_on_texts(sentence_train)\n",
    " \n",
    "# Menyimpan word_index kedalam sebuah file json\n",
    "word_index = tokenizer.word_index\n",
    " \n",
    "with open('word_index.json', 'w') as fp:\n",
    "    json.dump(word_index, fp)\n",
    " \n",
    "# Membuat sequences dan melakukan padding\n",
    "train_sekuens = tokenizer.texts_to_sequences(sentence_train)\n",
    "test_sekuens = tokenizer.texts_to_sequences(sentence_test)\n",
    " \n",
    "train_padded = pad_sequences(train_sekuens,\n",
    "                             maxlen=20,\n",
    "                             padding='post',\n",
    "                             truncating='post')\n",
    "test_padded = pad_sequences(test_sekuens,\n",
    "                            maxlen=20,\n",
    "                            padding='post',\n",
    "                            truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nadia/Documents/non-kuliah/CODINGCAMP/MachineLearningPlayGround/.venv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2025-04-03 00:28:46.418411: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.5484 - loss: 0.6923 - val_accuracy: 0.2400 - val_loss: 0.7238\n",
      "Epoch 2/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.5720 - loss: 0.6832 - val_accuracy: 0.2400 - val_loss: 0.7614\n",
      "Epoch 3/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.5595 - loss: 0.6810 - val_accuracy: 0.2400 - val_loss: 0.7688\n",
      "Epoch 4/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.5849 - loss: 0.6678 - val_accuracy: 0.2400 - val_loss: 0.7286\n",
      "Epoch 5/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5787 - loss: 0.6592 - val_accuracy: 0.4250 - val_loss: 0.6941\n",
      "Epoch 6/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7893 - loss: 0.6169 - val_accuracy: 0.4700 - val_loss: 0.6848\n",
      "Epoch 7/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8196 - loss: 0.5191 - val_accuracy: 0.8450 - val_loss: 0.5474\n",
      "Epoch 8/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8840 - loss: 0.3881 - val_accuracy: 0.8250 - val_loss: 0.4964\n",
      "Epoch 9/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9224 - loss: 0.2580 - val_accuracy: 0.8200 - val_loss: 0.4752\n",
      "Epoch 10/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9295 - loss: 0.2092 - val_accuracy: 0.6350 - val_loss: 0.6815\n",
      "Epoch 11/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9604 - loss: 0.1402 - val_accuracy: 0.6650 - val_loss: 0.6856\n",
      "Epoch 12/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9651 - loss: 0.1158 - val_accuracy: 0.6650 - val_loss: 0.7152\n",
      "Epoch 13/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9798 - loss: 0.0809 - val_accuracy: 0.7700 - val_loss: 0.6116\n",
      "Epoch 14/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9830 - loss: 0.0770 - val_accuracy: 0.6700 - val_loss: 0.7758\n",
      "Epoch 15/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9834 - loss: 0.0666 - val_accuracy: 0.7800 - val_loss: 0.6211\n",
      "Epoch 16/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9894 - loss: 0.0458 - val_accuracy: 0.7800 - val_loss: 0.6253\n",
      "Epoch 17/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9841 - loss: 0.0564 - val_accuracy: 0.7150 - val_loss: 0.8224\n",
      "Epoch 18/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9874 - loss: 0.0520 - val_accuracy: 0.7900 - val_loss: 0.6175\n",
      "Epoch 19/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9832 - loss: 0.0532 - val_accuracy: 0.7400 - val_loss: 0.8229\n",
      "Epoch 20/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9980 - loss: 0.0285 - val_accuracy: 0.7800 - val_loss: 0.7162\n",
      "Epoch 21/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9917 - loss: 0.0354 - val_accuracy: 0.7450 - val_loss: 0.8557\n",
      "Epoch 22/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9981 - loss: 0.0216 - val_accuracy: 0.6950 - val_loss: 0.9909\n",
      "Epoch 23/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9920 - loss: 0.0242 - val_accuracy: 0.7050 - val_loss: 0.9788\n",
      "Epoch 24/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9949 - loss: 0.0187 - val_accuracy: 0.7650 - val_loss: 0.8520\n",
      "Epoch 25/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9956 - loss: 0.0185 - val_accuracy: 0.7900 - val_loss: 0.7038\n",
      "Epoch 26/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9833 - loss: 0.0383 - val_accuracy: 0.7650 - val_loss: 0.8630\n",
      "Epoch 27/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9837 - loss: 0.0407 - val_accuracy: 0.7950 - val_loss: 0.7775\n",
      "Epoch 28/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9981 - loss: 0.0129 - val_accuracy: 0.7200 - val_loss: 1.0886\n",
      "Epoch 29/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9941 - loss: 0.0171 - val_accuracy: 0.7250 - val_loss: 1.0293\n",
      "Epoch 30/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9950 - loss: 0.0154 - val_accuracy: 0.7150 - val_loss: 1.1457\n"
     ]
    }
   ],
   "source": [
    "# Membuat model\n",
    "model = tf.keras.Sequential([\n",
    "    Embedding(2000, 20, input_length=20),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "    \n",
    "# Compile model\n",
    "model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "# Train model\n",
    "num_epochs = 30\n",
    "history = model.fit(train_padded, label_train,\n",
    "                    epochs=num_epochs,\n",
    "                    validation_data=(test_padded, label_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-03 00:30:50.075884: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743615050.124231   20864 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743615050.139328   20864 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743615050.179287   20864 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743615050.179366   20864 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743615050.179385   20864 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743615050.179395   20864 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-03 00:30:50.190776: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[32mğŸŒ² Try \u001b[0m\u001b[34mhttps://ydf.readthedocs.io\u001b[0m\u001b[32m, the successor of TensorFlow Decision Forests with more features and faster training!\u001b[0m\n",
      "failed to lookup keras version from the file,\n",
      "    this is likely a weight only file\n"
     ]
    }
   ],
   "source": [
    "# Convert model.h5 to model\n",
    "!tensorflowjs_converter --input_format=keras model.h5 tfjs_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menerapkan Model pada Web Browser dengan TensorFlow.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
